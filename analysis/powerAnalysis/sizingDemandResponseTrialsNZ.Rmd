---
params:
  author: 'Ben Anderson and Tom Rushby'
  title: 'Statistical Power, Statistical Significance, Study Design and Decision Making: A Worked Example'
  subtitle: 'Sizing Demand Response Trials in New Zealand'
title: '`r paste0(params$title)`'
subtitle: '`r paste0(params$subtitle)`'
author: '`r paste0(params$author)` (Contact: b.anderson@soton.ac.uk, `@dataknut`)'
date: 'Last run at: `r Sys.time()`'
output:
  bookdown::pdf_document2:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
  bookdown::html_document2:
    code_folding: hide
    fig_caption: yes
    keep_md: yes
    number_sections: yes
    self_contained: no
    toc: yes
    toc_depth: 2
    toc_float: yes
  bookdown::word_document2:
    fig_caption: yes
    toc: yes
    toc_depth: 2
bibliography: '`r path.expand("~/bibliography.bib")`'
---

```{r knitrSetup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # by default turn off code echo
```

```{r codeSetup, include=FALSE}
# Set start time ----
startTime <- proc.time()

library(GREENGridData) # local utilities
library(GREENGrid) # local utilities

# Set parameters etc ----
GREENGrid::setup()

# Packages needed in this .Rmd file ----
rmdLibs <- c("data.table", # data munching
             "dplyr", # data munching
             "ggplot2", # for fancy graphs
             "readr", # writing to files
             "lubridate", # for today
             "SAVEr", # power stats
             "kableExtra" # for extra kable
)
# load them
loadLibraries(rmdLibs)

# Local functions ---
labelProfilePlot <- function(plot){
  # adds neat labels to X axis
  plot <- plot + 
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 0.5)) +
    scale_x_time(breaks = c(hms::as.hms("00:00:00"),
                            hms::as.hms("04:00:00"), hms::as.hms("08:00:00"), 
                            hms::as.hms("12:00:00"), hms::as.hms("16:00:00"),
                            hms::as.hms("20:00:00"), hms::as.hms("24:00:00")))

  return(plot)
}

# Local parameters ----

dPath <- "~/Data/NZ_GREENGrid/reshare/v1.0/dataExtracts/" 
# created from https://dx.doi.org/10.5255/UKDA-SN-853334
# using https://github.com/CfSOtago/GREENGridData/blob/master/examples/code/extractCleanGridSpy1minCircuit.R
heatPumpData <- paste0(dPath, "Heat Pump_2015-04-01_2016-03-31_observations.csv.gz")
hotWaterData <- paste0(dPath, "Hot Water_2015-04-01_2016-03-31_observations.csv.gz")
lightingData <- paste0(dPath, "Lighting_2015-04-01_2016-03-31_observations.csv.gz")

plotCaption <- paste0("Source: ", ggParams$dataDOI)

# http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette
# with grey
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# with black
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

\newpage

# About

## Paper circulation:

 * Public

## License

```{r ccby license, child=ggParams$licenseCCBY}
```
 
## Citation

If you wish to use any of the material from this report please cite as:

 * `r params$author`. (`r lubridate::year(lubridate::now())`) `r params$title` (`r params$subtitle`), `r ggParams$pubLoc`.

This work is (c) `r lubridate::year(today())` the authors.

## History

Code history is generally tracked via our [repo](https://github.com/CfSOtago/GREENGrid):

 * [Report history](https://github.com/CfSOtago/GREENGrid/commits/master/analysis/powerAnalysis)
 
## Data:

This paper uses circuit level extracts for 'Heat Pumps', 'Lighting' and 'Hot Water' for the NZ GREEN Grid Household Electricity Demand Data (`r ggParams$dataDOI` [@anderson_new_2018]). These have been extracted using the code found in 

## Support

```{r generic support, child=ggParams$supportGeneric}
```
 
\newpage

# Introduction
In our experiennce of designing and running empirical studies, whether experimental or naturalistic, there is ongoing confusion over the meaning and role of two key statistical terms:
 
  * statistical power
  * statistical significance

We have found this to be the case both in academic research where the objective is to establish 'the most likely explanation' under academic conventions and in applied research where the objective is to 'make a robust decision' based on the balance of evidence and probability.

In this brief paper we respond to these confusions using a worked example: the design of a hypothetical household electricity demand response trial in New Zealand which seeks to shift the use of Heat Pumps out of the evening winter peak demand period. We use this example to explain and demonstrate the role of statistical signficance in testing for differences and of both statistical signficance and statistical power in sample design and decision making.

# Error, power, significance and decision making

Two types of error are of concern in both purely academic research where the efficacy of an intervention is to be tested and also in applied research where a decision may then be taken based on the results:

 * Type I: a false positive - an effect is inferred when in fact there is none. From a commercial or policy perspective this could lead to the implementation of a costly intervention which would be unlikely to have the effect expected;
 * Type II: a false negative - an effect is not inferred when in fact there is one. From a commercial or policy perspective this could lead to inaction when an intervention would have been likely to have the effect expected.
 
_Type I error_: The significance level (p value) of the statistical test to be used to test the efficacy of an intervention represents the extent to which the observed data matches the null model to be tested [@wasserstein2016]. In most trials the null model will be a measure of 'no difference' between control and intervenion groups. By convention, the p value _threshold_ for rejecting the null model (the risk of a Type I error) is generally set to 0.05 (5%) although this choice is entirely subjective. In commercial or policy terms an action taken on a larger p value (e.g. setting the p value threshold to 10%) would increase the risk of making a Type I error and thus implementing a potentially costly intervention that is unlikely to have the effect desired. However, as we discuss in more detail below, this is not necessarily _bad practice_ as it may reflect the potential magnitude of an effect, the decision-maker's tolerance of Type I error risk and the urgency of action.

_Type II error_: Statistical power is normally set to 0.8 (80%) by convention and represents the pre-study risk of making a Type II error [@Greenland2016]. From a commercial or policy perspective reducing power (e.g. to 0.7 or 70%) will therefore increase the risk of taking no action when in fact the intervention would probably have had the effect desired. Statistical power calculations enable the investigator to estimate the sample size that would be needed to robustly detect an experimental effect with a given risk of a false positive (Type I error) or false negative (Type II error) result. This prevents a study from recruiting too few participants to be able to robustly detect the hypothesised intervention effect [@Delmas2013Information] or wasting resources by recruiting a larger sample than needed. 

Previous work has suggested that sample sizes in most energy efficiency studies may be too low to provide adequate power and so statistically robust conclusions cannot be drawn at conventional thresholds [@Frederiks2016Evaluating] while a more recent review focusing on demand response studies reached a similar conclusion [@Srivastava2018Assessing]. It is therefore hardly surprising that a number of studies report effect sizes which are not statistically significant at conventional thresholds [@Srivastava2018Assessing], choose to use lower statistical significance thresholds [@RockyMountainInstitute2006Automated, @AECOM2011Energy, @CER2012Smart, @Schofield2015Experimental] or both lower statistical power values _and_ lower statistical significance thresholds [@energyWiseT1,@energyWiseT2].

However it would be wrong to conclude that this is _necessarily_ bad practice. Recent discussions of the role of p values in inference [@Greenland2016, @wasserstein2016] should remind us that decisions should never be based only on statistical significance thresholds set purely by convention. Rather, inference and thus decision making should be based on:

 * statistic effect size - is it 2% or 22% (i.e. is the result _important_ or _useful_, "What is the estimated _bang for buck_?");
 * statistic confidence intervals - (i.e. is there _uncertainty_ or _variation_ in response, "How uncertain is the estimated bang?");
 * statistic p values - (i.e. what is the risk of a Type I error / _false positive_, “What is the risk the bang observed isn’t real?”);

Only then can a contextually appropriate decision be taken as to whether the effect is large enough, certain enough and has a low enough risk of being a false positive result to warrant action.

In the following sections we apply these principles to the design and analysis of a hypothetical New Zealand household electricity demand response trial and to the use of a simple statistical test of difference between trial groups to demonstrate and clarify these points.

# Sample design: statistical power

To return to the discussion of statistical power, we need to establish the probably size of the control and intervention groups we will require. This is an aid to resource budgeting (_"How many households and thus `$` do I need?"_) and to ensure good study design practice ("_Will I be able to answer my research question?_") [@Frederiks2016Evaluating].

Calculation of the required sample size for a control and intervention group requires the estimation of the probable intervention effect size, agreement on the significance level (p value threshold or Type I error risk) of the statistical test to be used and agreement on the level of statistical power (Type II error risk). Given any three of these values the fourth can be calculated if an estimate of the mean and standard deviation of the outcome to be measured is known. In the case of DSR interventions the effect size comprises a given % reduction in energy demand or consumption in a given time period and estimates of the likely reduction can be derived from previous studies or data. 

As we have noted the choice of significance level (p value threshold) and statistical power are subjective and normative. Most academic researchers will struggle to justify relaxing from the conventional p = 0.05 and power = 0.8. However as we have discussed there may be good reason in applied research to take action on results of studies that use less conservative thresholds. Nevertheless there is a strong argument for designing such studies using the more conservative conventional levels but acknowledging that making inferences from the results may require a more relaxed approach to Type I or Type II error risks than is considered 'normal' in academic research.


```{r loadGgData, include =FALSE}
dt <- data.table::as.data.table(readr::read_csv(heatPumpData, progress = FALSE))

dt <- dt[, month := lubridate::month(r_dateTime)]
dt <- dt[, year := lubridate::year(r_dateTime)]

dt <- GREENGridData::addNZSeason(dt, r_dateTime)
```


```{r powerEstPrep}
testDT <- dt[lubridate::hour(r_dateTime) > 15 & # 16:00 ->
                        lubridate::hour(r_dateTime) < 20 & # <- 20:00
                        lubridate::wday(r_dateTime) != 6 & # not Saturday
                        lubridate::wday(r_dateTime) != 7 & # not Sunday
                        year == 2015,
                        .(meanW = mean(powerW, na.rm = TRUE)), keyby = .(season, linkID)]
```

```{r ggHPSampleSizeFig80, fig.cap="Power analysis results (power = 0.8)"}

makePowerPlot <- function(dt, myCaption){
  vLineAlpha <- 0.5
  vLineCol <- "#0072B2" # http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette
  
  yp001Ref <<- dt[pValue == "p = 0.01" & sampleN == 1000] # for reference line
  x001 <- mean(yp001Ref$sampleN)
  yp005Ref <- dt[pValue == "p = 0.05" & 
                       effectSize < ceiling(yp001Ref$effectSize) &
                       effectSize > floor(yp001Ref$effectSize)] # for reference line
  x005 <- mean(yp005Ref$sampleN)
  yp01Ref <- dt[pValue == "p = 0.1" & 
                      effectSize < ceiling(yp001Ref$effectSize) &
                      effectSize > floor(yp001Ref$effectSize)] # for reference line
  x01 <<- mean(yp01Ref$sampleN)
  yp02Ref <- dt[pValue == "p = 0.2" & 
                      effectSize < ceiling(yp001Ref$effectSize+1) & # fix this to get a value!
                      effectSize > floor(yp001Ref$effectSize-1)] # for reference line
  x02 <- mean(yp02Ref$sampleN)
  
  p <- ggplot2::ggplot(dt, aes(x = sampleN, y = effectSize, colour = pValue)) +
    geom_line() + 
    ylim(0,NA) +
    labs(x = "Sample size",
         y = "Detectable effect size (%)",
         caption = myCaption) +
    guides(colour = guide_legend(title = "p value: ")) +
    scale_colour_manual(values=cbPalette) + # use colour-blind friendly palette
    scale_y_continuous(breaks = seq(0,max(dt$effectSize),5)) + # add detail to y scale
    scale_x_continuous(breaks = seq(0,3000,200)) # add detail to x scale
  
  p <- p + 
    geom_hline(yintercept = yp001Ref$effectSize, alpha = vLineAlpha, colour = "black") +
    geom_segment(x = x001, y = yp001Ref$effectSize, xend = 1000, yend = 0, alpha = vLineAlpha,
                 colour = cbPalette[1]) +
    geom_segment(x = x005, y = yp001Ref$effectSize, xend = x005, yend = 0, alpha = vLineAlpha,
                 colour = cbPalette[2]) +
    geom_segment(x = x01, y = yp001Ref$effectSize, xend = x01, yend = 0, alpha = vLineAlpha,
                 colour = cbPalette[3]) + 
    geom_segment(x = x02, y = yp001Ref$effectSize, xend = x02, yend = 0, alpha = vLineAlpha,
                 colour = cbPalette[4])
  
  p <- p +
    annotate(geom = "text", 
             x = 1200, 
             y = yp001Ref$effectSize + 3, 
             label = paste0("Effect size = ", round(yp001Ref$effectSize, 2) ,"%"), 
             colour = cbPalette[1], 
             hjust = 0) # https://stackoverflow.com/questions/26684023/how-to-left-align-text-in-annotate-from-ggplot2
  return(p)
}

testMean <- mean(testDT[season == "Winter"]$meanW) 
testSD <- mean(testDT[season == "Winter"]$meanW) 
testSamples <- seq(50,3000,50)
testPower <- 0.8

powerRes80DT <- SAVEr::estimateEffectSizes(testMean,testSD,testSamples,testPower) # auto-produces range of p values



myCaption <- paste0(plotCaption, ", Winter 2015",
                    "\nStatistic: mean W, weekdays 16:00 - 20:00",
                    "\nTest: R function power.t.test, statistical power = 0.8")

p <- makePowerPlot(powerRes80DT, myCaption)
p

ggplot2::ggsave("figs/fig1_statPowerEsts80.png", p)

```

As an illustration, \ref(fig:ggHPSampleSizeFig80) shows sample size calculations for power = 0.8 (80%) using 'Heat Pump' electricity demand extracted from the publicly available New Zealand Green Grid household electricity demand data [@anderson_new_2018] for winter 2015 for the peak demand period (16:00 - 20:00) on weekdays.

These results show that a trial comprising a control and intervention sample of 1000 households (each) would be able to detect an effect size of `r round(yp001Ref$effectSize,2)`% with p = 0.01 and power = 0.8. Were a study to be less risk averse in it's decision making then p = 0.1 may be acceptable in which case only ~ `r x01` households would be needed in each group (see \ref(fig:ggHPSampleSizeFig80)) but the risk of a Type I error would increase. 

```{r ggHPSampleSizeFig70, fig.cap="Power analysis results (power = 0.7)"}

testPower <- 0.7

powerRes70DT <- SAVEr::estimateEffectSizes(testMean,testSD,testSamples,testPower) # auto-produces range of p values

myCaption <- paste0(plotCaption, ", Winter 2015",
                    "\nStatistic: mean W, weekdays 16:00 - 20:00",
                    "\nTest: R function power.t.test, statistical power = 0.7")

p <- makePowerPlot(powerRes70DT, myCaption)
p

ggplot2::ggsave("figs/fig1_statPowerEsts70.png", p)

```

Were we to reduce the statistical power to 0.7 then we would obtain the results shown in \ref(fig:ggHPSampleSizeFig70). In this case a trial comprising a control and intervention sample of 1000 households (each) would be able to detect an effect size of `r round(yp001Ref$effectSize,2)`% with p = 0.01 and power = 0.7. Were a study to be less risk averse in it's decision making then p = 0.1 may be acceptable in which case only ~ `r x01` households would be needed in each group (see \ref(fig:ggHPSampleSizeFig70)) but as before the risk of a Type I error would increase. Similarly, reducing the statistical power used would also reduce the sample required for a given effect size tested at a given p value. However, as before the risk of a Type II error would increase.

# Testing for differences: effect sizes, confidence intervals and p values

## Getting it 'wrong'

Let us imagine that we have not designed and implemented our sample recruitment according to \ref(fig:ggHPSampleSizeFig80) and instead decided, perhaps for cost reasons to recruit ~ 30 households per group. Now we wish to test for differences between the control and intervention groups.


```{r smallNTable}
testDT <- testDT[season == "Winter", group := "Control"]
testDT <- testDT[season == "Summer", group := "Intervention 1"]
testDT <- testDT[season == "Spring", group := "Intervention 2"]
testDT <- testDT[season == "Autumn", group := "Intervention 3"]

t <- testDT[, .("mean W" = mean(meanW),
                     "sd W" = sd(meanW),
                     "n households" = .N),
                 keyby = .(group)]

kableExtra::kable(t, caption = "Number of households and summary statistics per group") %>%
  kable_styling()
```
```{r ggMeanDiffs, fig.caption = "Mean W demand per group (Error bars = 95% confidence intervals for the sample mean)"}
plotDT <- testDT[, .(meanW = mean(meanW),
                     sdW = sd(meanW),
                     nObs = .N),
                 keyby = .(group)
                 ]

myCaption <- paste0("Hypothetical small n sample",
                    "\nStatistic: mean W, weekdays 16:00 - 20:00")

makeMeanCIPlot <- function(dt){
  # makes the plot, assumes meanW & sdW - not flexible but it does the job
  dt <- dt[, ci_upper := meanW + (qnorm(0.975) * sdW/sqrt(nObs))]
  dt <- dt[, ci_lower := meanW - (qnorm(0.975) * sdW/sqrt(nObs))]
  p <- ggplot2::ggplot(dt, aes(x = group, y = meanW, fill = group)) +
    geom_col() + 
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
    guides(fill = guide_legend(title = "Group")) +
    labs(y = "Mean W",
         x = "Trial group",
       caption = myCaption)
  return(p)
}

makeMeanCIPlot(plotDT)
  
```

As a first step we plot the differences using the mean and 95% confidence intervals as shown in \ref(fig:ggMeanDiffs). As we can see the interventions appear to have reduced demand quite substantially and the error bars indicate the uncertainty (variation) around the mean within each group. Based on this, we suspect that we are unlikely to see low p values when we use statistical tests of the differences as the error bars overlap substantially.

Suppose a t-test of the difference between the Control and Intervention 1 group produces the result shown below.
 
```{r tTestTabG1}
# fix
# we are going to compare winter with summer to get a large effect. This is not what we would really do as it is a repeat measures dataset but this is irrelevant for our current purposes.

tTest <- t.test(testDT[group == "Intervention 1"]$meanW, testDT[group == "Control"]$meanW)
tTest

controlW <- tTest$estimate[[2]]
intW <- tTest$estimate[[1]]

cil <- tTest$conf.int[[1]]
ciu <- tTest$conf.int[[2]]
```

The data shows that the mean power demand for the control group was `r round(controlW,2)`W and for Intervention 1 was `r round(intW,2)`W. This is a (very) large difference in the mean of `r round(controlW - intW, 2)`. The results of the t test are:

 * effect size = `r round(controlW - intW)`W or `r round(100 * (1-(intW/controlW)))`%  representing a _substantial bang for buck_ for whatever caused the difference;
 * 95% confidence interval for the test = `r round(cil,2)` to `r round(ciu,2)` representing _considerable_ uncertainty/variation;
 * p value of `r round(tTest$p.value,3)` representing a _relatively low_ risk of a false positive result but which (just) fails the conventional p < 0.05 threshold.
 
What would we have concluded? We have a large effect size, substantial uncertainty and a slightly raised risk of a false positive or Type I error when compared to conventional p value levels. From a narrow and conventional 'p value testing' perspective we would have concluded that there was no statistically signficant difference between the groups. However this misses the crucial point that an organisation with a higher risk tolerance might conclude that the large effect size justifies implementing the intervention even though the risk of a false positive is slightly higher. If the p value had been 0.25 then this would have still been the case but would have warranted even further caution.

But what about Intervention Group 2? In this case the t.test results are slightly different:

```{r tTestTabG2}
# fix
# now compare winter & spring for a smaller effect

tTest <- t.test(testDT[group == "Intervention 2"]$meanW, testDT[group == "Control"]$meanW)
tTest
controlW <- tTest$estimate[[2]]
intW <- tTest$estimate[[1]]
cil <- tTest$conf.int[[1]]
ciu <- tTest$conf.int[[2]]
```

Now:
 
 * effect size = `r round(controlW - intW)`W or `r round(100 * (1-(intW/controlW)),2)`%  representing a still _reasonable bang for buck_ for whatever caused the difference;
 * 95% confidence interval for the test = `r round(cil,2)` to `r round(ciu,2)` representing _even greater_ uncertainty/variation;
 * p value of `r round(tTest$p.value,3)` representing a _higher_ risk of a false positive result which fails the conventional p < 0.05 threshold and also the less conservative p < 0.1.

As before, the subsequent action we take depends on our tolerance of Type I (falso positive) risk. We still have a reasonably large effect size but we are less certain about it and we have a higher risk of it not being real. What do you think we should do?

```{r getN}
# get sample size required for Int Group 2
sd <- testDT[, sd(meanW)]
result <- power.t.test(
        n = NULL,
        delta = controlW - intW,
        sd = sd,
        sig.level = 0.05,
        power = 0.8,
        alternative = c("one.sided")
      )
```

In both cases our decision-making is rather hampered by the small sample size even though we have extremely large effect sizes. As we can see from \ref(fig:ggHPSampleSizeFig80), to detect Intervention Group 2's effect size of `r round(100 * (1-(intW/controlW)),2)`% would have required control and trial group sizes of `r round(result$n)` respectively.

However, as the recent discussions of the role of the p value in decision making have made clear [@wasserstein2016] statistical analysis needs to report all of the result elements to enable contextually appropriate and defensible evidence-based decisions to be taken. Simply dismissing results on the basis of a failure to meet conventional statistical levels of significance risks levitating babies and bathwater...

## Getting it 'right'

Suppose instead that we had designed and implemented our sample recruitment according to \ref(fig:ggHPSampleSizeFig80) so that we have a reasonable chance of detecting a difference of ~ 14% with power = 0.8 and at a significance level (p) of 0.05. This means we should have a sample of around 4000 households split equally (and randomly) between our trial and two intervention groups.

```{r creatLargeN}
# fix.
# we just randomly re-sample the GREEN Grid data
largeTestDT <- sample_frac(testDT, 40, replace = TRUE)

t <- largeTestDT[, .("mean W" = mean(meanW),
                     "sd W" = sd(meanW),
                     "n households" = .N),
                 keyby = .(group)]

kableExtra::kable(t, caption = "Number of households and summary statistics per group") %>%
  kable_styling()
```

```{r largeNmeanDiffs, fig.cap="Mean W demand per group for large sample (Error bars = 95% confidence intervals for the sample mean)"}
plotDT <- largeTestDT[, .(meanW = mean(meanW),
                     sdW = sd(meanW),
                     nObs = .N),
                 keyby = .(group)
                 ]

myCaption <- paste0("Hypothetical large n sample",
                    "\nStatistic: mean W, weekdays 16:00 - 20:00")
makeMeanCIPlot(plotDT)
```

In comparison to \ref(fig:ggMeanDiffs) we can now see (\ref(fig:largeNmeanDiffs)) that the 95% confidence intervals for the group means are much narrower. This is almost entirely due to the larger sample sizes. Re-running our previous test for differences now produces:

```{r largeNtTest1}
# now compare winter & spring for a smaller effect

tTest <- t.test(largeTestDT[group == "Intervention 2"]$meanW, largeTestDT[group == "Control"]$meanW)
tTest
controlW <- tTest$estimate[[2]]
intW <- tTest$estimate[[1]]
cil <- tTest$conf.int[[1]]
ciu <- tTest$conf.int[[2]]
```

In this case:

  * effect size = `r controlW - intW`W or `r round(100 * (1-(intW/controlW)),2)`%  representing a still _reasonable bang for buck_ for whatever caused the difference;
 * 95% confidence interval for the test = `r round(cil,2)` to `r round(ciu,2)` representing _much less_ uncertainty/variation;
 * p value of `r round(tTest$p.value,4)` representing a _very low_ risk of a false positive result as it passes all conventional thresholds.
 
So now we are able to be much more confident in our decision to implement Intervention 2 since the average effect is reasonably large, the expected variation in the effect size is reasonably narrow and the risk of a Type I (false positive) error is extremely small. 

# Summary and recomendations

## Statistical power and sample design

Get it right _first time_: we should do the statistical power analysis before we start to make sure the study is even worth trying. If we don't have previous data to use, we _justify_ our choices through power analysis based on defensible assumptions.

## Reporting statistical tests of difference (effects)

Report all three elements _always_:

 * average effect size
 * effect size confidence intervals
 * the p value (risk of Type I errors)

We should also report the statistical power used just to be clear on the risk of Type II errors.

## Making inferences and taking decisions

Pay attention to all three elements _always_:

 * average effect size: what is the _average bang for buck_?
 * effect size confidence intervals: _how uncertain is the bang_?
 * the p value: _what is the risk of a false positive_?

If we have ticked all the boxes so far then we have combined good study design based on statistical power analysis, with a nuanced understanding of what test statistic effect sizes, confidence intervals and p values can tell us. As a result we now have a robust, evidence-based, contextually meaningful and _defensible_ strategy.

# Ackowledgements

We would like to thank collaborators and partners on a number of applied research projects for prodding us into thinking about these issues more deeply and clearly than we othweise would have done. We hope this paper helps to bring some clarity.

# Runtime


```{r check runtime, include=FALSE}
t <- proc.time() - startTime

elapsed <- t[[3]]
```

Analysis completed in `r round(elapsed,2)` seconds ( `r round(elapsed/60,2)` minutes) using [knitr](https://cran.r-project.org/package=knitr) in [RStudio](http://www.rstudio.com) with `r R.version.string` running on `r R.version$platform`.

# R environment

R packages used:

 * base R - for the basics [@baseR]
 * data.table - for fast (big) data handling [@data.table]
 * lubridate - date manipulation [@lubridate]
 * ggplot2 - for slick graphics [@ggplot2]
 * readr - for csv reading/writing [@readr]
 * dplyr - for select and contains [@dplyr]
 * progress - for progress bars [@progress]
 * kableExtra - to create this document & neat tables [@knitr]
 * GREENGrid - for local NZ GREEN Grid project utilities

Session info:

```{r sessionInfo, echo=FALSE}
sessionInfo()
```

# References
