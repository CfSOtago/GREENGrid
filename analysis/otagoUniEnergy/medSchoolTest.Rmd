---
title: "Otago University Medical School Buildings Energy Consumption Data"
author: "Ben Anderson (b.anderson@soton.ac.uk, `@dataknut`), Carsten Dortans (carsten.dortans@web.de)"
date: 'Last run at: `r Sys.time()`'
output:
  bookdown::html_document2:
    self_contained: no
    toc: yes
    toc_float: yes
#bibliography: '`r path.expand("~/bibliography.bib")`'
---

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Set start time ----
startTime <- proc.time()

# Libraries ----
library(data.table)
library(ggplot2)
library(hms)
library(kableExtra)
library(lubridate)
library(plotly)
library(skimr)

# Data file ----
# make platform independent

dFile <- "/Volumes/hum-csafe/Research Projects/GREEN Grid/externalData/OtagoUni/MedicalSchoolBuildingsOtago.csv" # default

# if BA
userName <- Sys.info()[[7]]

if(userName == "ben"){
  dFile <- "~/Data/OtagoUni/MedicalSchoolBuildings.csv"
}
```

# Load data

Use `data.table::fread()` to load the data. It deals with the `;` delimiter automatically.

Loadng data from `dFile`, reporting any warnings and listing the columns/variables:

```{r loadData}

dt <- data.table::fread(path.expand(dFile), 
                  )
message("N rows:", nrow(dt))
names(dt)
```

Describe the raw data:

```{r skimData}
skimr::skim(dt)
```

# Clean and process original data

Keep the vars we want and test.

```{r keepVars}
# keep the vars we want

cleanDT <- dt[, .(Time,
                  avg = `Consumed Active Energy Total Tariff Sum L1-L3 (15m) [D20X Whole Medical School]-avg[Wh]`,
                  min = `Consumed Active Energy Total Tariff Sum L1-L3 (15m) [D20X Whole Medical School]-min[Wh]`,
                  max = `Consumed Active Energy Total Tariff Sum L1-L3 (15m) [D20X Whole Medical School]-max[Wh]`)]

message("Keeping: ")
names(cleanDT)
```

Now clean the data. During this process we force R to think the timezone of the original `Time` variable is NZ Time. This is important because otherwise R might (sometimes) assume it is UTC and we would then get DST break errors and incorrect daily profiles.

```{r cleanData}

cleanDT[, dateTime := lubridate::dmy_hms(Time)] # is this UTC? No we think it's NZ time
cleanDT[, dateTime := lubridate::force_tz(dateTime, tzone = "Pacific/Auckland")]
cleanDT[, hour := lubridate::hour(dateTime)]

cleanDT[, hms := hms::as.hms(dateTime)]
cleanDT[, date := lubridate::date(dateTime)]
cleanDT[, dow := lubridate::wday(date, label = TRUE)]
cleanDT[, month := lubridate::month(date, label = TRUE)]

# check
t <- head(cleanDT[, .(Time, dateTime, hour)])

kableExtra::kable(t, caption = "Testing conversion of Time variable")

```

Now summarise and test consumption.

```{r testClean}
s <- summary(cleanDT)

kableExtra::kable(s, caption = "Summary of data") %>%
  kable_styling()

ggplot2::ggplot(cleanDT, aes(x = dateTime)) +
  geom_line(aes(y = avg))
```

So clearly we have cumulative observations. 

# Test data

Next we calculate the difference making sure the data is sorted by date time and repeat the plot.

```{r getEnergy, fig.cap="Average electricity consumption per 15 minute period" }
setkey(cleanDT, dateTime)
cleanDT[, avgCons := avg - shift(avg)]


p <- ggplot2::ggplot(cleanDT, aes(x = dateTime)) +
  geom_line(aes(y = avgCons))

p

plotly::ggplotly(p) # interactive version
```

Hmm, clearly some spikes where data has 'caught up'. We need to check where these data holes might be. The first set of plots summarise the data by date.


```{r byDate}
plotDT <- cleanDT[, .(nObs = .N,
                      meanCons = mean(avgCons)), keyby = .(date)]
plotDT[, dow := lubridate::wday(date, label = TRUE)]

p <- ggplot2::ggplot(plotDT, aes(x = date, colour = dow)) + 
  geom_point(aes(y = nObs))
plotly::ggplotly(p)


p <- ggplot2::ggplot(plotDT, aes(x = date, colour = dow)) + 
  geom_point(aes(y = meanCons))
plotly::ggplotly(p)

```

Next we look by date and hour of the day.

```{r byDateHour}
plotDT <- cleanDT[, .(nObs = .N,
                      meanCons = mean(avgCons)), keyby = .(date, hour)]

myCaption <- "Non-shaded dates indicate missing observations"

ggplot2::ggplot(plotDT, aes(x = date, y = hour, alpha = nObs)) + geom_tile()+
  labs(x = "Date",
       y = "Hour",
       caption = myCaption) +
  guides(alpha = guide_legend(title = "N Obs"))

ggplot2::ggplot(plotDT, aes(x = date, y = hour, alpha = meanCons/1000)) + # kWh
  geom_tile() +
  labs(x = "Date",
       y = "Hour",
       caption = myCaption) +
  guides(alpha = guide_legend(title = "Average kWh"))


```

And finally we try to plot all the data to see which observations stand out.

```{r byDateHMS}

ggplot2::ggplot(cleanDT, aes(x = date, y = hms, alpha = avgCons/1000)) + #kWh
  geom_tile() +
  labs(x = "Date",
       y = "Time of day") +
  guides(alpha = guide_legend(title = "Average kWh"))

```

Whats going on? Firstly:

 * 7 Apr 2019 - Daylight Saving Time Ended - this will effect observations around 02:00 on Sunday 7th April when there will be a whole hour 'catch up'. So 8 observations will be allocated to the DST break hour. This means that there will be a mini-spike in apparent consumption;
 * 18 Feb 2019 - most likely a data outage causes a 'catch-up' spike (see Figure \@ref(fig:getEnergy))

# Demand profile plots

No data cleaning here. Should remove the 18th Feb data point...

```{r profilePlots, fig.height=8, fig.cap="Monthly consumption profile plots"}

plotDT <- cleanDT[, .(nObs = .N,
                      meanCons = mean(avgCons)), keyby = .(hms, dow, month)]

ggplot2::ggplot(plotDT, aes(x = hms, y = meanCons/1000, colour = dow)) + 
  geom_point() +
  facet_grid(month ~ .) +
  labs(y = "kWh per 15 minutes") +
  guides(colour = guide_legend(title = "Day of the week"))

```

And now with the outlier removed...

```{r profilePlotsClean, fig.height=6, fig.cap="Monthly consumption profile plots (Feb 2019 outlier removed)"}

# just aggregate the data which is less than the max value
plotDT <- cleanDT[avgCons < max(avgCons, na.rm = TRUE), # watch out for the NA
                  .(nObs = .N,
                      meanCons = mean(avgCons)), 
                  keyby = .(hms, dow, month)]

ggplot2::ggplot(plotDT, aes(x = hms, y = meanCons/1000, colour = dow)) + 
  geom_point() +
  facet_grid(month ~ .) +
  labs(y = "kWh per 15 minutes") +
  guides(colour = guide_legend(title = "Day of the week"))

```

# Runtime


```{r check runtime, include=FALSE}
t <- proc.time() - startTime

elapsed <- t[[3]]
```

Analysis completed in `r round(elapsed,2)` seconds ( `r round(elapsed/60,2)` minutes) using [knitr](https://cran.r-project.org/package=knitr) in [RStudio](http://www.rstudio.com) with `r R.version.string` running on `r R.version$platform`.

# R environment

R packages used:

 * base R - for the basics [@baseR]
 * bookdown - to make the report [@bookdown]
 * data.table - for fast (big) data handling [@data.table]
 * hms - clock time [@hms]
 * lubridate - date manipulation [@lubridate]
 * ggplot2 - for slick graphics [@ggplot2]
 * kableExtra - neat tables [@kableExtra]
 * plotly - interactive plots [@plotly]

Session info:

```{r sessionInfo, echo=FALSE}
sessionInfo()
```

# References
